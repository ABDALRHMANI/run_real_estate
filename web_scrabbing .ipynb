{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFlO_Xd1dIvK",
        "outputId": "c78c0b61-137c-4d57-ce03-b5c765aaae73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2550\n",
            "<Response [200]>\n",
            "25\n",
            "https://www.propertyfinder.eg/en/plp/buy/apartment-for-sale-cairo-new-cairo-city-south-investors-area-90-avenue-5279236.html\n",
            "not ok\n",
            "some_detail_text 2 Bedrooms2 Bathrooms 1,776 sqft / 165 sqm\n",
            "desc Apartment for Sale in 90 Avenue Compound, Fifth Settlement90th Street in front of the American University directly (AUC)The apartment is fully finished ultra-modernBUA: 165 sqm (2 bedrooms + 2 bathrooms) with a direct view of the poolCash down payment: (50%)and the rest over 3 years without interest.For details: 01062525558Or via WhatsApp: [link not available]------------------------------------------------------------------About Tabarak Real Estate Development:---------------------------------------------For more than 37 years, we have been working with dedication and loyalty. We have successfully transitioned from a single business unit to a holding company.Throughout our journey of work and success, the group has embodied excellence in all its endeavors, where credibility and mastery have accompanied its values, commitment and high professionalism. Tabarak has gained the respect of all business partners and customers.--------------------------------------------------------------90 Avenue Compound:------------------------------It occupies a strategic location in New Cairo, overlooking the American University in Cairo and adjacent to the Future University.The project is surrounded by a variety of shopping malls, cafes, restaurants, hospitals and schools.90 Avenue Compound also features underground parking, a gym, a club, a cycling track, and indoor and outdoor swimming pools. Homeowners will also have access to dedicated barbecue areas and a children's area.90 Avenue Compound is a great choice for those looking for a luxurious and comfortable life in the Fifth Settlement. The project offers a variety of residential units to suit all needs and desires, and it enjoys a strategic location close to all services and facilities.\n",
            "loc 90 AvenueCairo, New Cairo City, South Investors Area\n",
            "dev Directions Real Estate Consultancy(1231 Properties)\n",
            "price 7,000,000 EGP\n",
            "de######################################### https://apps.mapbox.com/feedback/?owner=propertyfindermap&id=cju6mojts1tkj1fmnoxcuyrgp&access_token=pk.eyJ1IjoicHJvcGVydHlmaW5kZXJtYXAiLCJhIjoiY2prdXN0ZGwwMDl5NjNwbXFkOHlvMWFxYiJ9.0UGHSOknu7qt3oXgvbhIzg#/31.49403/30.02469/13/-14.6/22\n",
            "Latitude: 31.49403\n",
            "Longitude: 30.02469\n",
            "hd########## {'Property Type': 'Apartment', 'Bedrooms': ' 2', 'Bathrooms': ' 2', 'Property Size': ' 1,776 sqft / 165 sqm'}\n",
            "pd######### {'Payment method': 'Cash'}\n"
          ]
        }
      ],
      "source": [
        "import geopy as geopy\n",
        "import requests\n",
        "import selenium as selenium\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "\n",
        "def handle_location_geo(url):\n",
        "    driver = None\n",
        "    try:\n",
        "        # Set Chrome options to run headless and add custom headers\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        chrome_options.add_argument(\n",
        "            \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\")\n",
        "\n",
        "        # Create WebDriver instance with custom options\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "        # Load the URL in the driver\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait for the button to be present in the DOM\n",
        "        button = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.XPATH, \"//*[@id='root_element']/main/div[2]/div/div/button[2]\")))\n",
        "\n",
        "        # Click the button\n",
        "        button.click()\n",
        "\n",
        "        # Wait for the new page to load\n",
        "        WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
        "            (By.XPATH, '//*[@id=\"portal-root\"]/div/div/div/div/div/div[3]/div[4]/div/div/a[3]')))\n",
        "\n",
        "        # Now you can parse the updated page source with BeautifulSoup\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "        de = soup.select(\n",
        "            '#portal-root > div > div > div > div > div > div.mapboxgl-control-container > div.mapboxgl-ctrl-bottom-right > div > div > a.mapbox-improve-map')\n",
        "        print(\"de#########################################\", de[0]['href'])\n",
        "\n",
        "        # Split the URL at the '#' character and take the second part\n",
        "        coords_part = de[0]['href'].split('#/')[1]\n",
        "\n",
        "        # Split the coordinates part at the '/' character and take the first two parts\n",
        "        latitude, longitude = coords_part.split('/')[:2]\n",
        "\n",
        "        print(\"Latitude:\", latitude)\n",
        "        print(\"Longitude:\", longitude)\n",
        "        return latitude, longitude\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None, None\n",
        "\n",
        "    finally:\n",
        "        if driver is not None:\n",
        "            driver.quit()\n",
        "\n",
        "\n",
        "def page_handle(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
        "    }\n",
        "\n",
        "    page = requests.get(url, headers=headers)\n",
        "    src = page.content\n",
        "    soup = BeautifulSoup(src, \"html.parser\")\n",
        "    details = soup.find_all(\"div\", {\"class\": \"styles_desktop_list__item__lF_Fh\"})\n",
        "    if len(details) > 2:\n",
        "        details = soup.find_all(\"div\", {\"class\": \"styles_desktop_list__item__lF_Fh\"})\n",
        "        descreption = soup.find_all(\"article\", {\"data-testid\": True})\n",
        "        location = soup.find_all(\"div\", {\"class\": \"styles_desktop_subtitle__container__msqhF\"})\n",
        "        development = soup.find_all(\"div\", {\"class\": [\"styles_desktop_broker__name__container__Hsrhs\"]})\n",
        "        price = soup.find(\"div\", {\"class\": [\"styles_desktop_price__row__kV3nj\"]})\n",
        "        pay_det = soup.find_all(\"div\", {\"class\": [\"styles_desktop_down-payment__box__D6mcQ\"]})\n",
        "        some_detail=soup.find(\"div\", {\"class\": [\"styles_attributes__list__sTe9h\"]})\n",
        "        some_detail_text=some_detail.text if some_detail else \"unknown\"\n",
        "        # Find the <img> tag inside the URL\n",
        "        # Find the button\n",
        "        description_text = descreption[0].text if descreption else \"unknown\"\n",
        "        location_text = location[0].text if location else \"unknown\"\n",
        "        development_text = development[0].text if development else \"unknown\"\n",
        "        print(\"desc\", description_text)\n",
        "        print(\"loc\", location_text)\n",
        "        print(\"dev\", development_text)\n",
        "        print(\"price\", price.text)\n",
        "        det=[]\n",
        "        hd=get_house_details(details)\n",
        "        return hd, description_text, location_text, development_text, price.text, pay_det\n",
        "    else:\n",
        "      print(\"not ok\")\n",
        "      details = soup.find_all(\"div\", {\"class\": \"styles_desktop_list__item__lF_Fh\"})\n",
        "      descreption = soup.find_all(\"article\", {\"data-testid\": True})\n",
        "      location = soup.find_all(\"div\", {\"class\": \"styles_desktop_subtitle__container__msqhF\"})\n",
        "      development = soup.find_all(\"div\", {\"class\": [\"styles_desktop_broker__name__container__Hsrhs\"]})\n",
        "      price = soup.find(\"div\", {\"class\": [\"styles_desktop_price__m_hT3 styles_attributes__price-container__jlc6e\"]})\n",
        "      pay_det = soup.find_all(\"div\", {\"class\": [\"styles_desktop_down-payment__box__D6mcQ\"]})\n",
        "      some_detail=soup.find(\"div\", {\"class\": [\"styles_attributes__list__sTe9h\"]})\n",
        "\n",
        "      description_text = descreption[0].text if descreption else \"unknown\"\n",
        "      location_text = location[0].text if location else \"unknown\"\n",
        "      development_text = development[0].text if development else \"unknown\"\n",
        "      price_text = price.text if price else \"unknown\"\n",
        "      some_detail_text=some_detail.text if some_detail else \"unknown\"\n",
        "      print(\"some_detail_text\",some_detail_text)\n",
        "      print(\"desc\", description_text)\n",
        "      print(\"loc\", location_text)\n",
        "      print(\"dev\", development_text)\n",
        "      print(\"price\", price_text)\n",
        "      det=[]\n",
        "      det.append(details[0].text)\n",
        "\n",
        "      # Use regular expressions to extract the details\n",
        "      pattern = r'(\\d+) Bedrooms( \\+ Maid)?(\\d+) Bathrooms ([\\d,]+) sqft / ([\\d,]+) sqm'\n",
        "      match = re.search(pattern, some_detail_text)\n",
        "\n",
        "      if match:\n",
        "          bedrooms, maid, bathrooms, sqft, sqm = match.groups()\n",
        "\n",
        "          # If '+ Maid' is present, increment the bedroom count\n",
        "          if maid:\n",
        "              bedrooms = str(int(bedrooms) + 1)\n",
        "\n",
        "          # Format the details in the desired format\n",
        "          det.append(f\"Bedrooms: {bedrooms}\")\n",
        "          det.append(f\"Bathrooms: {bathrooms}\")\n",
        "          det.append(f\"Property Size: {sqft} sqft / {sqm} sqm\")\n",
        "      house_dt=get_house_details_not(det)\n",
        "      return house_dt, description_text, location_text, development_text, price_text, pay_det\n",
        "\n",
        "\n",
        "def get_house_details(details):\n",
        "    house_details = {}\n",
        "    for i in range(len(details)):\n",
        "        all_text = details[i].text.split(':')\n",
        "        key = all_text[0]\n",
        "        value = all_text[1]\n",
        "        house_details[key] = value\n",
        "\n",
        "    return house_details\n",
        "def get_house_details_not(details):\n",
        "    house_details = {}\n",
        "    for i in range(len(details)):\n",
        "        all_text = details[i].split(':')\n",
        "        key = all_text[0]\n",
        "        value = all_text[1]\n",
        "        house_details[key] = value\n",
        "\n",
        "    return house_details\n",
        "\n",
        "def get_house_payment_details(details):\n",
        "    payment_details = {}\n",
        "    for i in range(len(details)):\n",
        "        if 'Payment method' in details[i].text:\n",
        "            value = details[i].text.split('Payment method')[1]\n",
        "            payment_details['Payment method'] = value\n",
        "        else:\n",
        "            value = details[i].text.split('Down payment')[1]\n",
        "            payment_details['Down payment'] = value\n",
        "    return payment_details\n",
        "\n",
        "\n",
        "def handle_location_name(location):\n",
        "    loc = location.split(',')\n",
        "    compound = loc[0]\n",
        "    city = loc[1]\n",
        "    return compound, city\n",
        "\n",
        "\n",
        "def get_all_pages(main_page):\n",
        "    src = main_page.content\n",
        "    soup = BeautifulSoup(src, \"html.parser\")\n",
        "    # Find all <a> tags with a title attribute\n",
        "    links = soup.find_all('ul', {\"class\": \"styles_desktop_container__V85pq\"})\n",
        "    # Use a set to store unique URLs\n",
        "    unique_urls = set()\n",
        "\n",
        "    # Loop through the found links and extract URLs\n",
        "    for link in links[0].find_all_next('li'):\n",
        "\n",
        "        url = link.find_next('a')['href']\n",
        "        # Check if the URL ends with .html\n",
        "        if url.endswith(\".html\") and url.startswith(\"https\"):\n",
        "            unique_urls.add(url)\n",
        "    return list(unique_urls)\n",
        "\n",
        "\n",
        "details = []\n",
        "latl = []\n",
        "longl = []\n",
        "descl = []\n",
        "devl = []\n",
        "locl = []\n",
        "all_urls = []\n",
        "payment = []\n",
        "pricel = []\n",
        "for i in range(2550, 2551):\n",
        "    print(i)\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"\n",
        "    }\n",
        "\n",
        "    main_page = requests.get(f\"https://www.propertyfinder.eg/en/search?c=1&fu=0&ob=mr&page={i}\", headers=headers)\n",
        "    print(main_page)\n",
        "\n",
        "    unique_urls = get_all_pages(main_page)\n",
        "    print(len(unique_urls))\n",
        "    count = 0\n",
        "    for u in unique_urls:\n",
        "        print(u)\n",
        "        count += 1\n",
        "        hd, des, loc, dev, price, pay_det = page_handle(u)\n",
        "        lat, long1 = handle_location_geo(u)\n",
        "        pd = get_house_payment_details(pay_det)\n",
        "        print(\"hd##########\", hd)\n",
        "        print(\"pd#########\", pd)\n",
        "        details.append(hd)\n",
        "        latl.append(lat)\n",
        "        longl.append(long1)\n",
        "        descl.append(des)\n",
        "        devl.append(dev)\n",
        "        locl.append(loc)\n",
        "        all_urls.append(u)\n",
        "        payment.append(pd)\n",
        "        pricel.append(price)\n",
        "        if count==1:\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-7IJ6RLdZ6k",
        "outputId": "0a685039-14db-4fd0-fdce-c7192e239ef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows written: 1175\n",
            "Data has been saved to locations_real_c1_t_2550to_2597.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import itertools\n",
        "# Extract all keys from all dictionaries\n",
        "all_keys_details = set().union(*(d.keys() for d in details))\n",
        "all_keys_payment = set().union(*(p.keys() for p in payment))\n",
        "\n",
        "# Combine all keys from details and payment\n",
        "all_keys = all_keys_details.union(all_keys_payment)\n",
        "\n",
        "# Define the CSV file name\n",
        "csv_file = \"locations_real_c1_t_2550to_2597.csv\"\n",
        "\n",
        "# Specify encoding for Arabic text\n",
        "encoding = 'utf-8-sig'\n",
        "\n",
        "# Convert all_keys to a list and concatenate it with the additional column names\n",
        "fieldnames = list(all_keys) + ['compound', 'dev', 'latitude', 'longitude', 'description', 'url', 'price']\n",
        "\n",
        "# Initialize counter\n",
        "count = 0\n",
        "\n",
        "# Write the data to the CSV file\n",
        "with open(csv_file, mode='w', newline='', encoding=encoding) as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    # Write header\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write data\n",
        "    for lat, long1, desc, u, house_details, comp,dev, price, pay in itertools.zip_longest(latl, longl, descl, all_urls, details, locl, devl, pricel, payment):\n",
        "        try:\n",
        "            # Fill in missing keys with None\n",
        "            if house_details is None:\n",
        "                house_details = {key: None for key in all_keys}\n",
        "            if pay is None:\n",
        "                pay = {key: None for key in all_keys}\n",
        "\n",
        "            row = {**house_details, **pay, 'compound': comp, 'dev': dev, 'latitude': lat, 'longitude': long1, 'description': desc, 'url': u, 'price': price}\n",
        "\n",
        "            # Write the row to the CSV filewww\n",
        "            writer.writerow(row)\n",
        "\n",
        "            # Increment counter\n",
        "            count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error writing row: {e}\")\n",
        "\n",
        "# Print the total number of rows written\n",
        "print(f\"Total rows written: {count}\")\n",
        "\n",
        "# Print the name of the CSV file where data has been saved\n",
        "print(\"Data has been saved to\", csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjqdRcMaeNDC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}